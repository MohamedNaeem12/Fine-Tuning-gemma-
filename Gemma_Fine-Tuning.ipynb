{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMafsqlsMXv99DQscFo7biV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"OxV9eDVVYT__","executionInfo":{"status":"error","timestamp":1767202498137,"user_tz":-120,"elapsed":45,"user":{"displayName":"mohamed naeem","userId":"02253371019009304552"}},"outputId":"6e28db90-4158-48bf-9e28-e99449a4e07e"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (ipython-input-3003098181.py, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3003098181.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install -U \\\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["!pip3 install -q -U pip\n","!apt-get install -y rustc\n","!pip3 install -q -U bitsandbytes==0.42.0\n","!pip3 install -q -U accelerate==0.27.1\n","!pip3 install -q -U peft==0.8.2\n","!pip3 install -q -U trl\n","!pip3 install -q -U datasets\n","!pip3 install -q -U transformers"]},{"cell_type":"code","source":["\n","!pip3 install -q -U pip# Upgrade pip\n","!apt-get install -y rustc# Install Rust (needed for tokenizers if build is required)\n","\n","!pip3 install -q -U \\# Install compatible LLM fine-tuning stack (LoRA / PEFT / TRL)\n","\n","  transformers==4.38.2 \\ #For loading the model and tokenizer\n","  peft==0.8.2 \\ #Efficient fine-tunig(LoRA)\n","  accelerate==0.27.1 \\ #Multi-GPU/ mixed precision\n","  datasets==2.17.0 \\ #Loading datasets\n","  bitsandbytes==0.42.0 \\ #Loaddin 8bit and 4bits\n","  trl==0.7.10 #Training library\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"yMyjciNWe595","executionInfo":{"status":"error","timestamp":1767203012176,"user_tz":-120,"elapsed":24,"user":{"displayName":"mohamed naeem","userId":"02253371019009304552"}},"outputId":"131825d4-2baa-4211-b6a9-999acda95c6b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"unexpected indent (ipython-input-2777100723.py, line 6)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2777100723.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    transformers==4.38.2 \\ #For loading the model and tokenizer\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]},{"cell_type":"code","source":["import os\n","import torch #For CUDA kernals/low-level speed\n","from datasets import load_dataset\n","from google.colab import userdata\n","import transformers\n","from trl import SFTTrainer  #For traning loop\n","from peft import LoraConfig   #For fine-tuning using LoRA\n","from transformers import AuotoTokenizer, AutoModelForCausalLM\n","from transformers import BitsAndBytesConfig, GemmaTokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"xumcP3TZYuDW","executionInfo":{"status":"error","timestamp":1767201961086,"user_tz":-120,"elapsed":20778,"user":{"displayName":"mohamed naeem","userId":"02253371019009304552"}},"outputId":"4d2eec1c-5857-4350-c1e8-3c03959a8c6e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'AuotoTokenizer' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2744929949.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAuotoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGemmaTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'AuotoTokenizer' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["os.environ['HUGGINGFACE_TOKEN'] = userdata.get('HUGGINGFACE_TOKEN')"],"metadata":{"id":"HItoV9VrYt9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_id= \"ggogle/gemma-2b\"\n","#For fine-tuning trnsforming 32bit to 4bits using bnb config\n","bnb_config = BitsAndBytesConfig(\n","  load_in_4bit=True,\n","  bnb_4bit_quant_type=\"nf4\",\n","  bnb_4bit_compute_dtype=torch.bfloat16\n","  )"],"metadata":{"id":"9fTnoxXQYrsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer= AutoTokenizer.from_pretrained(model_id, token=os.environ['HUGGINGFACE_TOKEN'])\n","model = AutoModelForCausalLM.from_pretrained(model_id,\n","                                             quantization_config=bnb_config,\n","                                             dvice_map={\"\":0}, #llm run on may devices and devicemao helo us to shoose where to run (if device_map = auto that's mean auto split layres nif =0 mean put everything on gpu 0)\n","                                            token=os.environ['HUGGINGFACE_TOKEN']\n","                                             )\n"],"metadata":{"id":"xxQysrFvjQLN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=\"Quote: Imaginatons is more,\"\n","device = \"cuda:0\"\n","inputs=tokneizer(text, return_tensts=\"pt\").to(device)\n","outputs= model.generate(**inputs, max_new_tokens=20)\n","print(toknizer.decode(outputs[0], skip_special_tokens=True))\n","#"],"metadata":{"id":"7KgBiKDP6pfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_config = LoraConfig(\n","    r=8,\n"," atgre_modules = [\"q_proj\", \"o_proj\", \"k_proj\",\"\"gate_proj\", \"up_proj\",\"\"down_proj\",\"v_proj\"],\n","    task_tyoe = \"CAUSAL_LM\"\n",")"],"metadata":{"id":"oRs6H3a090R8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","data = load_dataset(\"Abirate/english_quotes\")\n","data= data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"],"metadata":{"id":"QO4dx2-t-pzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['train']['quote']"],"metadata":{"id":"kmiVWDd8-pxC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def formatting_func(example):\n","  text = f\"Quote: {example['quote'][0]}\"\\nAuthor: {example['author'][0]}\"\n","  return text"],"metadata":{"id":"6SxYe_lu-puf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = SFTTrainer(\n","    model=model,\n","    args =transformers.TrainingArguments(\n","      per_device_train_batch_size=1,\n","      gradient_accumulation_steps=4,\n","      warmup_steps=2,\n","      max_steps=100,\n","      learning_rate=2e-4,\n","      fp16=True,\n","      logging_steps=1,\n","      output_dir=\"output\",\n","      optim=\"paged_adamw_8bit\",\n","    ),\n","    peft_config = lora_config,\n","    formatting_func=formatting_func,\n","\n","    )\n","\n"],"metadata":{"id":"UvRzp-_w-psO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tranier.train()"],"metadata":{"id":"cvWqeArX-pp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text=\"Quote: Imaginatons is more,\"\n","device = \"cuda:0\"\n","inputs=tokneizer(text, return_tensts=\"pt\").to(device)\n","outputs= model.generate(**inputs, max_new_tokens=20)\n","print(toknizer.decode(outputs[0], skip_special_tokens=True))\n","#"],"metadata":{"id":"pJB5AaJZMFBv"},"execution_count":null,"outputs":[]}]}